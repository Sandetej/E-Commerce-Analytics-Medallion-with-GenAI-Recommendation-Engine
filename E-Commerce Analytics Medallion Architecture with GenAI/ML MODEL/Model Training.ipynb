{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a9bc486-1218-4ea7-bb5a-c952a99db234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Feature Engineering**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b9ec025-2f06-44f0-af86-22def222ff17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/sundarasandeepteja@gmail.com/E-Commerce Analytics Medallion Architecture with GenAI/config/project_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565f1e84-0b93-48f3-ab0f-8ba32146cd8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ======================================\n",
    "# ML: FEATURE ENGINEERING FOR RECOMMENDATIONS\n",
    "# ======================================\n",
    "\n",
    "# MAGIC %run ../config/project_config\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "print(\"ü§ñ ML: Feature Engineering for Recommendation Engine\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ======================================\n",
    "# STEP 1: LOAD SOURCE DATA\n",
    "# ======================================\n",
    "print(\"\\nüì• Step 1: Loading source data...\")\n",
    "\n",
    "transactions = spark.table(SILVER_TRANSACTIONS_TABLE).filter(F.col(\"status\") == \"Completed\")\n",
    "print(f\"  Transactions: {transactions.count():,}\")\n",
    "\n",
    "ratings = spark.table(f\"{SILVER_DB}.ratings_validated\")\n",
    "print(f\"  Ratings: {ratings.count():,}\")\n",
    "\n",
    "products = spark.table(GOLD_DIM_PRODUCTS_TABLE)\n",
    "print(f\"  Products: {products.count():,}\")\n",
    "\n",
    "customers = spark.table(GOLD_DIM_CUSTOMERS_TABLE)\n",
    "print(f\"  Customers: {customers.count():,}\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 2: CREATE PURCHASE FEATURES\n",
    "# ======================================\n",
    "print(\"\\nüõí Step 2: Creating purchase features...\")\n",
    "\n",
    "purchase_features = transactions.groupBy(\"customer_id\", \"product_id\").agg(\n",
    "    F.count(\"transaction_id\").alias(\"purchase_count\"),\n",
    "    F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "    F.sum(\"final_amount\").alias(\"total_spent\"),\n",
    "    F.avg(\"final_amount\").alias(\"avg_order_value\"),\n",
    "    F.min(\"transaction_date\").alias(\"first_purchase_date\"),\n",
    "    F.max(\"transaction_date\").alias(\"last_purchase_date\"),\n",
    "    F.countDistinct(\"transaction_date\").alias(\"purchase_days\")\n",
    ")\n",
    "print(f\"  Purchase pairs: {purchase_features.count():,}\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 3: CREATE RATING FEATURES\n",
    "# ======================================\n",
    "print(\"\\n‚≠ê Step 3: Creating rating features...\")\n",
    "\n",
    "rating_window = Window.partitionBy(\"customer_id\", \"product_id\").orderBy(F.desc(\"rating_date\"))\n",
    "rating_features = ratings.withColumn(\"rn\", F.row_number().over(rating_window)).filter(F.col(\"rn\") == 1).select(\n",
    "    \"customer_id\", \"product_id\", \"rating\", \"has_review\", \"helpful_votes\", \"sentiment\", \"rating_date\"\n",
    ")\n",
    "print(f\"  Rating pairs: {rating_features.count():,}\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 4: COMBINE INTERACTIONS\n",
    "# ======================================\n",
    "print(\"\\nüîó Step 4: Combining purchase and rating data...\")\n",
    "\n",
    "interactions = purchase_features.join(rating_features, on=[\"customer_id\", \"product_id\"], how=\"full_outer\")\n",
    "interactions = interactions.fillna({\n",
    "    \"purchase_count\": 0,\n",
    "    \"total_quantity\": 0,\n",
    "    \"total_spent\": 0,\n",
    "    \"rating\": 0,\n",
    "    \"has_review\": False,\n",
    "    \"helpful_votes\": 0\n",
    "})\n",
    "print(f\"  Total interactions: {interactions.count():,}\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 5: CREATE IMPLICIT SCORE\n",
    "# ======================================\n",
    "print(\"\\nüìä Step 5: Creating implicit score...\")\n",
    "\n",
    "interactions = interactions.withColumn(\n",
    "    \"purchase_score\", F.least(F.col(\"purchase_count\"), F.lit(10)) / 10 * 5\n",
    ").withColumn(\n",
    "    \"recency_score\",\n",
    "    F.when(F.col(\"last_purchase_date\").isNotNull(),\n",
    "        F.when(F.datediff(F.current_date(), F.col(\"last_purchase_date\")) <= 30, 5)\n",
    "         .when(F.datediff(F.current_date(), F.col(\"last_purchase_date\")) <= 90, 4)\n",
    "         .when(F.datediff(F.current_date(), F.col(\"last_purchase_date\")) <= 180, 3)\n",
    "         .when(F.datediff(F.current_date(), F.col(\"last_purchase_date\")) <= 365, 2)\n",
    "         .otherwise(1)\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"quantity_score\", F.least(F.col(\"total_quantity\"), F.lit(20)) / 20 * 5\n",
    ")\n",
    "\n",
    "interactions = interactions.withColumn(\n",
    "    \"implicit_score\",\n",
    "    F.when(F.col(\"rating\") > 0,\n",
    "        F.col(\"rating\") * 0.4 +\n",
    "        F.col(\"purchase_score\") * 0.3 +\n",
    "        F.col(\"recency_score\") * 0.2 +\n",
    "        F.col(\"quantity_score\") * 0.1\n",
    "    ).otherwise(\n",
    "        F.col(\"purchase_score\") * 0.5 +\n",
    "        F.col(\"recency_score\") * 0.3 +\n",
    "        F.col(\"quantity_score\") * 0.2\n",
    "    )\n",
    ")\n",
    "\n",
    "interactions = interactions.withColumn(\n",
    "    \"implicit_score\", F.greatest(F.least(F.col(\"implicit_score\"), F.lit(5)), F.lit(0.5))\n",
    ")\n",
    "print(\"  ‚úÖ Implicit score calculated\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 6: CREATE NUMERIC IDS FOR ALS (HASH-BASED)\n",
    "# ======================================\n",
    "print(\"\\nüî¢ Step 6: Creating numeric IDs for ALS...\")\n",
    "\n",
    "interactions = interactions.withColumn(\n",
    "    \"customer_idx\", F.abs(F.hash(F.col(\"customer_id\")))\n",
    ").withColumn(\n",
    "    \"product_idx\", F.abs(F.hash(F.col(\"product_id\")))\n",
    ")\n",
    "print(f\"  Unique customers: {interactions.select('customer_idx').distinct().count():,}\")\n",
    "print(f\"  Unique products: {interactions.select('product_idx').distinct().count():,}\")\n",
    "\n",
    "ml_features = interactions\n",
    "\n",
    "# ======================================\n",
    "# STEP 7: ADD CONTEXTUAL FEATURES\n",
    "# ======================================\n",
    "print(\"\\nüè∑Ô∏è Step 7: Adding contextual features...\")\n",
    "\n",
    "ml_features = ml_features.join(\n",
    "    customers.select(\"customer_id\", \"clv_segment\", \"region\", \"age_group\"),\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "ml_features = ml_features.join(\n",
    "    products.select(\"product_id\", \"category\", \"price_tier\", \"brand\"),\n",
    "    on=\"product_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(\"  ‚úÖ Context features added\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 8: SAVE FEATURES AND MAPPINGS\n",
    "# ======================================\n",
    "print(\"\\nüíæ Step 8: Saving features and mappings...\")\n",
    "\n",
    "final_features = ml_features.select(\n",
    "    \"customer_id\", \"product_id\", \"customer_idx\", \"product_idx\",\n",
    "    \"implicit_score\", \"rating\", \"purchase_score\", \"recency_score\", \"quantity_score\",\n",
    "    \"purchase_count\", \"total_quantity\", \"total_spent\",\n",
    "    \"clv_segment\", \"region\", \"age_group\", \"category\", \"price_tier\", \"brand\",\n",
    "    \"first_purchase_date\", \"last_purchase_date\"\n",
    ")\n",
    "\n",
    "final_features.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{SILVER_DB}.ml_interaction_features\")\n",
    "print(f\"  ‚úÖ Saved: {SILVER_DB}.ml_interaction_features\")\n",
    "\n",
    "customer_mapping = ml_features.select(\"customer_id\", \"customer_idx\").distinct()\n",
    "customer_mapping.write.format(\"delta\").mode(\"overwrite\").save(f\"{ML_PATH}/customer_mapping\")\n",
    "print(f\"  ‚úÖ Saved: {ML_PATH}/customer_mapping\")\n",
    "\n",
    "product_mapping = ml_features.select(\"product_id\", \"product_idx\").distinct()\n",
    "product_mapping.write.format(\"delta\").mode(\"overwrite\").save(f\"{ML_PATH}/product_mapping\")\n",
    "print(f\"  ‚úÖ Saved: {ML_PATH}/product_mapping\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 9: SUMMARY STATISTICS\n",
    "# ======================================\n",
    "print(\"\\nüìä Step 9: Feature Summary...\")\n",
    "\n",
    "display(final_features.select(\"implicit_score\").summary())\n",
    "\n",
    "display(\n",
    "    final_features.groupBy(\"clv_segment\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"interactions\"),\n",
    "        F.round(F.avg(\"implicit_score\"), 2).alias(\"avg_score\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"interactions\"))\n",
    ")\n",
    "\n",
    "display(\n",
    "    final_features.groupBy(\"category\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"interactions\"),\n",
    "        F.round(F.avg(\"implicit_score\"), 2).alias(\"avg_score\")\n",
    "    )\n",
    "    .orderBy(F.desc(\"interactions\"))\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ü§ñ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59071d5c-159b-48ad-885f-837c8e1b71df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Model Training with MLflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6300591-d92f-4e5f-a4de-9311dcc2dcd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ======================================\n",
    "# ML: TRAIN ALS RECOMMENDATION MODEL\n",
    "# ======================================\n",
    "\n",
    "# MAGIC %run ../config/project_config\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ü§ñ ML: Training ALS Recommendation Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ======================================\n",
    "# STEP 1: LOAD ML FEATURES\n",
    "# ======================================\n",
    "print(\"\\nüì• Step 1: Loading ML features...\")\n",
    "\n",
    "ml_data = spark.table(f\"{SILVER_DB}.ml_interaction_features\")\n",
    "\n",
    "als_data = ml_data.select(\n",
    "    \"customer_idx\",\n",
    "    \"product_idx\", \n",
    "    \"implicit_score\",\n",
    "    \"customer_id\",\n",
    "    \"product_id\"\n",
    ").filter(\n",
    "    F.col(\"customer_idx\").isNotNull() &\n",
    "    F.col(\"product_idx\").isNotNull() &\n",
    "    F.col(\"implicit_score\").isNotNull()\n",
    ")\n",
    "\n",
    "print(f\"  Total interactions: {als_data.count():,}\")\n",
    "print(f\"  Unique customers: {als_data.select('customer_idx').distinct().count():,}\")\n",
    "print(f\"  Unique products: {als_data.select('product_idx').distinct().count():,}\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 2: TRAIN/TEST SPLIT\n",
    "# ======================================\n",
    "print(\"\\nüìä Step 2: Splitting data...\")\n",
    "\n",
    "train_data, test_data = als_data.randomSplit(\n",
    "    [TRAIN_TEST_SPLIT, 1 - TRAIN_TEST_SPLIT], \n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"  Training set: {train_data.count():,}\")\n",
    "print(f\"  Test set: {test_data.count():,}\")\n",
    "\n",
    "# Remove cache for serverless\n",
    "# train_data.cache()\n",
    "# test_data.cache()\n",
    "\n",
    "# ======================================\n",
    "# STEP 3: SETUP MLFLOW\n",
    "# ======================================\n",
    "print(\"\\nüî¨ Step 3: Setting up MLflow...\")\n",
    "\n",
    "experiment_name = \"/ecommerce-recommendation-engine\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"  Experiment: {experiment_name}\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 4: TRAIN ALS MODEL\n",
    "# ======================================\n",
    "print(\"\\nüèãÔ∏è Step 4: Training ALS model...\")\n",
    "\n",
    "# ... (previous code unchanged)\n",
    "if mlflow.active_run():\n",
    "    mlflow.end_run()\n",
    "run_name = f\"als_run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    mlflow.log_param(\"algorithm\", \"ALS\")\n",
    "    mlflow.log_param(\"max_iter\", ALS_MAX_ITER)\n",
    "    mlflow.log_param(\"reg_param\", ALS_REG_PARAM)\n",
    "    mlflow.log_param(\"rank\", ALS_RANK)\n",
    "    mlflow.log_param(\"train_size\", train_data.count())\n",
    "    mlflow.log_param(\"test_size\", test_data.count())\n",
    "    mlflow.log_param(\"implicit_prefs\", False)\n",
    "    mlflow.log_param(\"cold_start_strategy\", \"drop\")\n",
    "    \n",
    "    als = ALS(\n",
    "        maxIter=ALS_MAX_ITER,\n",
    "        regParam=ALS_REG_PARAM,\n",
    "        rank=ALS_RANK,\n",
    "        userCol=\"customer_idx\",\n",
    "        itemCol=\"product_idx\",\n",
    "        ratingCol=\"implicit_score\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True,\n",
    "        seed=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    print(\"  Training model...\")\n",
    "    model = als.fit(train_data)\n",
    "    print(\"  ‚úÖ Model trained\")\n",
    "    \n",
    "    print(\"  Generating predictions...\")\n",
    "    predictions = model.transform(test_data)\n",
    "    predictions = predictions.filter(F.col(\"prediction\").isNotNull())\n",
    "    \n",
    "    print(\"\\nüìä Step 5: Evaluating model...\")\n",
    "    \n",
    "    rmse_evaluator = RegressionEvaluator(\n",
    "        metricName=\"rmse\",\n",
    "        labelCol=\"implicit_score\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    rmse = rmse_evaluator.evaluate(predictions)\n",
    "    \n",
    "    mae_evaluator = RegressionEvaluator(\n",
    "        metricName=\"mae\",\n",
    "        labelCol=\"implicit_score\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    mae = mae_evaluator.evaluate(predictions)\n",
    "    \n",
    "    r2_evaluator = RegressionEvaluator(\n",
    "        metricName=\"r2\",\n",
    "        labelCol=\"implicit_score\",\n",
    "        predictionCol=\"prediction\"\n",
    "    )\n",
    "    r2 = r2_evaluator.evaluate(predictions)\n",
    "    \n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"mae\", mae)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "    \n",
    "    print(f\"\\n  üìà Model Performance:\")\n",
    "    print(f\"     RMSE: {rmse:.4f}\")\n",
    "    print(f\"     MAE:  {mae:.4f}\")\n",
    "    print(f\"     R¬≤:   {r2:.4f}\")\n",
    "    \n",
    "    print(\"\\nüíæ Step 6: Saving model...\")\n",
    "\n",
    "    from mlflow.models.signature import infer_signature\n",
    "\n",
    "    signature = infer_signature(\n",
    "        train_data.select(\"customer_idx\", \"product_idx\", \"implicit_score\").toPandas(),\n",
    "        model.transform(train_data).select(\"prediction\").toPandas()\n",
    "    )\n",
    "\n",
    "    input_example = train_data.select(\"customer_idx\", \"product_idx\", \"implicit_score\").limit(1).toPandas()\n",
    "\n",
    "    mlflow.spark.log_model(\n",
    "        model,\n",
    "        \"als_model\",\n",
    "        registered_model_name=\"workspace.ecommerce_silver.ecommerce_recommendation_als\",\n",
    "        dfs_tmpdir=\"/Volumes/workspace/ecommerce_silver/ml_models/tmp\",\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "\n",
    "    model_path = f\"{ML_PATH}/als_model\"\n",
    "    model.write().overwrite().save(model_path)\n",
    "    mlflow.log_param(\"model_path\", model_path)\n",
    "    print(f\"  ‚úÖ Model saved to: {model_path}\")\n",
    "    print(f\"  ‚úÖ Model logged to MLflow\")\n",
    "    run_id = run.info.run_id\n",
    "    print(f\"  üìù MLflow Run ID: {run_id}\")\n",
    "\n",
    "print(\"\\nüìã Step 7: Sample Predictions...\")\n",
    "\n",
    "predictions.select(\n",
    "    \"customer_idx\", \"product_idx\", \"implicit_score\", \"prediction\"\n",
    ").withColumn(\n",
    "    \"error\", F.abs(F.col(\"implicit_score\") - F.col(\"prediction\"))\n",
    ").orderBy(\n",
    "    F.desc(\"implicit_score\")\n",
    ").limit(20)\n",
    "\n",
    "display(predictions)\n",
    "\n",
    "# Remove unpersist for serverless\n",
    "# train_data.unpersist()\n",
    "# test_data.unpersist()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ü§ñ ALS MODEL TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e0dec7-cd1f-4789-a0cf-19f19f3c4aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"SPARKML_TEMP_DFS_PATH\"] = \"/Volumes/workspace/ecommerce_silver/ml_models/tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "361c61a0-3600-4bb4-a143-6ddf780ba110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ======================================\n",
    "# ML: HYPERPARAMETER TUNING\n",
    "# ======================================\n",
    "\n",
    "# MAGIC %run ../config/project_config\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import mlflow\n",
    "\n",
    "print(\"ü§ñ ML: Hyperparameter Tuning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data\n",
    "ml_data = spark.table(f\"{SILVER_DB}.ml_interaction_features\") \\\n",
    "    .select(\"customer_idx\", \"product_idx\", \"implicit_score\") \\\n",
    "    .filter(F.col(\"customer_idx\").isNotNull())\n",
    "\n",
    "train_data, test_data = ml_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set: {train_data.count():,}\")\n",
    "\n",
    "# ======================================\n",
    "# SETUP GRID SEARCH\n",
    "# ======================================\n",
    "print(\"\\nüîß Setting up hyperparameter grid...\")\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"customer_idx\",\n",
    "    itemCol=\"product_idx\",\n",
    "    ratingCol=\"implicit_score\",\n",
    "    coldStartStrategy=\"drop\",\n",
    "    nonnegative=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(als.rank, [10, 20, 30]) \\\n",
    "    .addGrid(als.maxIter, [10, 15]) \\\n",
    "    .addGrid(als.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .build()\n",
    "\n",
    "print(f\"  Total combinations: {len(param_grid)}\")\n",
    "\n",
    "# Evaluator\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"implicit_score\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=als,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# ======================================\n",
    "# RUN CROSS-VALIDATION\n",
    "# ======================================\n",
    "print(\"\\nüèãÔ∏è Running cross-validation (this may take a while)...\")\n",
    "\n",
    "mlflow.set_experiment(\"/ecommerce-recommendation-tuning\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"als_hyperparameter_tuning\"):\n",
    "    \n",
    "    # Fit CV\n",
    "    cv_model = cv.fit(train_data)\n",
    "    \n",
    "    # Best model\n",
    "    best_model = cv_model.bestModel\n",
    "    \n",
    "    # Best parameters\n",
    "    # Best parameters\n",
    "    best_rank = best_model.rank\n",
    "    best_reg = best_model.regParam\n",
    "    best_iter = best_model.maxIter\n",
    "\n",
    "    print(f\"\\nüìä Best Parameters:\")\n",
    "    print(f\"   Rank: {best_rank}\")\n",
    "    print(f\"   RegParam: {best_reg}\")\n",
    "    print(f\"   MaxIter: {best_iter}\")\n",
    "    \n",
    "    # Evaluate best model\n",
    "    predictions = best_model.transform(test_data)\n",
    "    rmse = evaluator.evaluate(predictions)\n",
    "    \n",
    "    print(f\"\\nüìà Best Model RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    # Log to MLflow\n",
    "    mlflow.log_param(\"best_rank\", best_rank)\n",
    "    mlflow.log_param(\"best_reg_param\", best_reg)\n",
    "    mlflow.log_param(\"best_max_iter\", best_iter)\n",
    "    mlflow.log_metric(\"best_rmse\", rmse)\n",
    "    \n",
    "    # Save best model\n",
    "    best_model.write().overwrite().save(f\"{ML_PATH}/als_model_tuned\")\n",
    "    mlflow.spark.log_model(best_model, \"best_als_model\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best model saved to: {ML_PATH}/als_model_tuned\")\n",
    "\n",
    "train_data.unpersist()\n",
    "\n",
    "print(\"\\nü§ñ HYPERPARAMETER TUNING COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7654218759511057,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Model Training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
