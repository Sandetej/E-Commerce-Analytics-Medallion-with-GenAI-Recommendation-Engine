{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c9dd40-48bc-4b3f-9067-7ef09c466eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# BRONZE LAYER: TRANSACTION INGESTION\n",
    "# MAGIC %run ../config/project_config\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3706bfe6-d7c5-4f28-be25-7fe7c9be64a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/sundarasandeepteja@gmail.com/E-Commerce Analytics Medallion Architecture with GenAI/config/project_config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf96b2a9-4bab-4eca-9088-9935d468ac53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"‚ñ† BRONZE LAYER: Transaction Ingestion\")\n",
    "print(\"=\" * 50)\n",
    "# Read raw data\n",
    "raw_txn = spark.read.parquet(f\"{RAW_DATA_PATH}/transactions\")\n",
    "print(f\"Raw records: {raw_txn.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91fdf777-f10f-4ab4-a90d-a5599fcdf800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Add metadata columns\n",
    "batch_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "bronze_txn = raw_txn.withColumns({\n",
    "    \"_ingested_at\": F.current_timestamp(),\n",
    "    \"_source_file\": F.lit(f\"{RAW_DATA_PATH}/transactions\"),\n",
    "    \"_batch_id\": F.lit(batch_id),\n",
    "    \"_ingestion_date\": F.current_date(),\n",
    "    \"_row_hash\": F.md5(F.concat_ws(\"|\",\n",
    "    F.col(\"transaction_id\"),\n",
    "    F.col(\"customer_id\"),\n",
    "    F.col(\"final_amount\").cast(\"string\")\n",
    " ))\n",
    "})\n",
    "print(f\"Added {5} metadata columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c227d9e1-4741-468e-a0c1-50437670cc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Write to Delta Lake\n",
    "table_exists = spark.catalog.tableExists(BRONZE_TRANSACTIONS_TABLE)\n",
    "bronze_path = \"/Volumes/workspace/default/ecommerce_project_volume/bronze/\"\n",
    "if not table_exists:\n",
    "    bronze_txn.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"_ingestion_date\") \\\n",
    "        .option(\"path\", bronze_path) \\\n",
    "        .saveAsTable(BRONZE_TRANSACTIONS_TABLE)\n",
    "    print(f\"‚ñ† Created: {BRONZE_TRANSACTIONS_TABLE} at {bronze_path}\")\n",
    "else:\n",
    "    # Merge for incremental loads using table name\n",
    "    delta_table = DeltaTable.forName(spark, BRONZE_TRANSACTIONS_TABLE)\n",
    "    # List all columns except the key\n",
    "    update_columns = [col for col in bronze_txn.columns if col != \"transaction_id\"]\n",
    "    set_dict = {col: f\"source.{col}\" for col in update_columns}\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        bronze_txn.alias(\"source\"),\n",
    "        \"target.transaction_id = source.transaction_id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"target._row_hash != source._row_hash\",\n",
    "        set=set_dict\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "    print(f\"‚ñ† Merged into: {BRONZE_TRANSACTIONS_TABLE} at {bronze_path}\")\n",
    "\n",
    "# Optimize\n",
    "spark.sql(f\"OPTIMIZE {BRONZE_TRANSACTIONS_TABLE} ZORDER BY (customer_id, product_id)\")\n",
    "display(spark.table(BRONZE_TRANSACTIONS_TABLE).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d21522-ed34-432a-bf66-95d3cfdc9dd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_products = spark.read.parquet(f\"{RAW_DATA_PATH}/products\")\n",
    "batch_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "bronze_products = raw_products.withColumns({\n",
    "                        \"_ingested_at\": F.current_timestamp(),\n",
    "                        \"_batch_id\": F.lit(batch_id),\n",
    "                        \"_ingestion_date\": F.current_date(),\n",
    "                    })\n",
    "bronze_products.write.format(\"delta\").mode(\"overwrite\") \\\n",
    " .saveAsTable(BRONZE_PRODUCTS_TABLE)\n",
    "print(f\"‚ñ† Created: {BRONZE_PRODUCTS_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e3132d1-cc04-4907-9c95-4017b45aac92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ======================================\n",
    "# BRONZE LAYER: CUSTOMER INGESTION\n",
    "# ======================================\n",
    "\n",
    "# MAGIC %run ../config/project_config\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ü•â BRONZE LAYER: Customer Ingestion\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ======================================\n",
    "# STEP 1: READ RAW DATA\n",
    "# ======================================\n",
    "print(\"\\nüì• Step 1: Reading raw customer data...\")\n",
    "\n",
    "raw_customers = spark.read.parquet(f\"{RAW_DATA_PATH}/customers\")\n",
    "raw_count = raw_customers.count()\n",
    "\n",
    "print(f\"  Records found: {raw_count:,}\")\n",
    "print(f\"  Columns: {len(raw_customers.columns)}\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 2: ADD METADATA COLUMNS\n",
    "# ======================================\n",
    "print(\"\\nüè∑Ô∏è Step 2: Adding metadata columns...\")\n",
    "\n",
    "batch_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "bronze_customers = raw_customers.withColumns({\n",
    "    # Ingestion timestamp\n",
    "    \"_ingested_at\": F.current_timestamp(),\n",
    "    \n",
    "    # Source file path\n",
    "    \"_source_file\": F.lit(f\"{RAW_DATA_PATH}/customers\"),\n",
    "    \n",
    "    # Batch identifier\n",
    "    \"_batch_id\": F.lit(batch_id),\n",
    "    \n",
    "    # Ingestion date (for partitioning)\n",
    "    \"_ingestion_date\": F.current_date(),\n",
    "    \n",
    "    # Row hash for change detection\n",
    "    \"_row_hash\": F.md5(F.concat_ws(\"|\",\n",
    "        F.col(\"customer_id\"),\n",
    "        F.col(\"email\"),\n",
    "        F.col(\"phone\"),\n",
    "        F.col(\"segment\"),\n",
    "        F.col(\"state\"),\n",
    "        F.col(\"is_active\").cast(\"string\")\n",
    "    )),\n",
    "    \n",
    "    # PII flag - marks this table contains sensitive data\n",
    "    \"_contains_pii\": F.lit(True),\n",
    "    \n",
    "    # Processing status\n",
    "    \"_is_processed\": F.lit(False)\n",
    "})\n",
    "\n",
    "print(f\"  ‚úÖ Added 7 metadata columns\")\n",
    "print(f\"  ‚úÖ PII flag set to True\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 3: WRITE TO DELTA LAKE\n",
    "# ======================================\n",
    "print(\"\\nüíæ Step 3: Writing to Delta Lake...\")\n",
    "\n",
    "table_exists = spark.catalog.tableExists(BRONZE_CUSTOMERS_TABLE)\n",
    "\n",
    "if not table_exists:\n",
    "    print(\"  Creating new Bronze table...\")\n",
    "    \n",
    "    bronze_customers.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"_ingestion_date\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(BRONZE_CUSTOMERS_TABLE)\n",
    "    \n",
    "    print(f\"  ‚úÖ Created table: {BRONZE_CUSTOMERS_TABLE}\")\n",
    "    \n",
    "else:\n",
    "    print(\"  Table exists - performing MERGE operation...\")\n",
    "    \n",
    "    delta_table = DeltaTable.forName(spark, BRONZE_CUSTOMERS_TABLE)\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        bronze_customers.alias(\"source\"),\n",
    "        \"target.customer_id = source.customer_id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"target._row_hash != source._row_hash\",\n",
    "        set={\n",
    "            \"first_name\": \"source.first_name\",\n",
    "            \"last_name\": \"source.last_name\",\n",
    "            \"email\": \"source.email\",\n",
    "            \"phone\": \"source.phone\",\n",
    "            \"state\": \"source.state\",\n",
    "            \"segment\": \"source.segment\",\n",
    "            \"registration_date\": \"source.registration_date\",\n",
    "            \"birth_year\": \"source.birth_year\",\n",
    "            \"is_active\": \"source.is_active\",\n",
    "            \"_ingested_at\": \"source._ingested_at\",\n",
    "            \"_source_file\": \"source._source_file\",\n",
    "            \"_batch_id\": \"source._batch_id\",\n",
    "            \"_row_hash\": \"source._row_hash\",\n",
    "            \"_contains_pii\": \"source._contains_pii\",\n",
    "            \"_is_processed\": F.lit(False)\n",
    "        }\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "    \n",
    "    print(\"  ‚úÖ Merge operation complete\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 4: OPTIMIZE WITH ZORDER\n",
    "# ======================================\n",
    "print(\"\\n‚ö° Step 4: Optimizing table...\")\n",
    "\n",
    "spark.sql(f\"OPTIMIZE {BRONZE_CUSTOMERS_TABLE} ZORDER BY (segment, state)\")\n",
    "\n",
    "print(\"  ‚úÖ Optimized with ZORDER on (segment, state)\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 5: VERIFY AND SUMMARIZE\n",
    "# ======================================\n",
    "print(\"\\n‚úÖ Step 5: Verification...\")\n",
    "\n",
    "final_count = spark.table(BRONZE_CUSTOMERS_TABLE).count()\n",
    "print(f\"  Total records in Bronze: {final_count:,}\")\n",
    "\n",
    "# Show segment distribution\n",
    "print(\"\\nüìä Segment Distribution:\")\n",
    "spark.table(BRONZE_CUSTOMERS_TABLE) \\\n",
    "    .groupBy(\"segment\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .show()\n",
    "\n",
    "# Show state distribution (top 5)\n",
    "print(\"üìä Top 5 States:\")\n",
    "spark.table(BRONZE_CUSTOMERS_TABLE) \\\n",
    "    .groupBy(\"state\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(F.desc(\"count\")) \\\n",
    "    .limit(5) \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ü•â BRONZE CUSTOMER INGESTION COMPLETE!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260aa56a-f5a6-4d8f-9c2b-a896a89ee4b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ======================================\n",
    "# BRONZE LAYER: RATINGS INGESTION\n",
    "# ======================================\n",
    "\n",
    "# MAGIC %run ../config/project_config\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"ü•â BRONZE LAYER: Ratings Ingestion\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ======================================\n",
    "# STEP 1: READ RAW DATA\n",
    "# ======================================\n",
    "print(\"\\nüì• Step 1: Reading raw ratings data...\")\n",
    "\n",
    "raw_ratings = spark.read.parquet(f\"{RAW_DATA_PATH}/ratings\")\n",
    "raw_count = raw_ratings.count()\n",
    "\n",
    "print(f\"  Records found: {raw_count:,}\")\n",
    "print(f\"  Columns: {len(raw_ratings.columns)}\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 2: ADD METADATA COLUMNS\n",
    "# ======================================\n",
    "print(\"\\nüè∑Ô∏è Step 2: Adding metadata columns...\")\n",
    "\n",
    "batch_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "bronze_ratings = raw_ratings.withColumns({\n",
    "    # Ingestion timestamp\n",
    "    \"_ingested_at\": F.current_timestamp(),\n",
    "    \n",
    "    # Source file path\n",
    "    \"_source_file\": F.lit(f\"{RAW_DATA_PATH}/ratings\"),\n",
    "    \n",
    "    # Batch identifier\n",
    "    \"_batch_id\": F.lit(batch_id),\n",
    "    \n",
    "    # Ingestion date (for partitioning)\n",
    "    \"_ingestion_date\": F.current_date(),\n",
    "    \n",
    "    # Row hash for change detection\n",
    "    \"_row_hash\": F.md5(F.concat_ws(\"|\",\n",
    "        F.col(\"rating_id\"),\n",
    "        F.col(\"customer_id\"),\n",
    "        F.col(\"product_id\"),\n",
    "        F.col(\"rating\").cast(\"string\"),\n",
    "        F.col(\"helpful_votes\").cast(\"string\")\n",
    "    )),\n",
    "    \n",
    "    # Processing status\n",
    "    \"_is_processed\": F.lit(False)\n",
    "})\n",
    "\n",
    "print(f\"  ‚úÖ Added 6 metadata columns\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 3: WRITE TO DELTA LAKE\n",
    "# ======================================\n",
    "print(\"\\nüíæ Step 3: Writing to Delta Lake...\")\n",
    "\n",
    "table_exists = spark.catalog.tableExists(BRONZE_RATINGS_TABLE)\n",
    "\n",
    "if not table_exists:\n",
    "    print(\"  Creating new Bronze table...\")\n",
    "    \n",
    "    bronze_ratings.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"_ingestion_date\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(BRONZE_RATINGS_TABLE)\n",
    "    \n",
    "    print(f\"  ‚úÖ Created table: {BRONZE_RATINGS_TABLE}\")\n",
    "    \n",
    "else:\n",
    "    print(\"  Table exists - performing MERGE operation...\")\n",
    "    \n",
    "    delta_table = DeltaTable.forName(spark, BRONZE_RATINGS_TABLE)\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        bronze_ratings.alias(\"source\"),\n",
    "        \"target.rating_id = source.rating_id\"\n",
    "    ).whenMatchedUpdate(\n",
    "        condition=\"target._row_hash != source._row_hash\",\n",
    "        set={\n",
    "            \"customer_id\": \"source.customer_id\",\n",
    "            \"product_id\": \"source.product_id\",\n",
    "            \"rating\": \"source.rating\",\n",
    "            \"has_review\": \"source.has_review\",\n",
    "            \"rating_date\": \"source.rating_date\",\n",
    "            \"helpful_votes\": \"source.helpful_votes\",\n",
    "            \"_ingested_at\": \"source._ingested_at\",\n",
    "            \"_source_file\": \"source._source_file\",\n",
    "            \"_batch_id\": \"source._batch_id\",\n",
    "            \"_row_hash\": \"source._row_hash\",\n",
    "            \"_is_processed\": F.lit(False)\n",
    "        }\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "    \n",
    "    print(\"  ‚úÖ Merge operation complete\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 4: OPTIMIZE WITH ZORDER\n",
    "# ======================================\n",
    "print(\"\\n‚ö° Step 4: Optimizing table...\")\n",
    "\n",
    "spark.sql(f\"OPTIMIZE {BRONZE_RATINGS_TABLE} ZORDER BY (product_id, customer_id)\")\n",
    "\n",
    "print(\"  ‚úÖ Optimized with ZORDER on (product_id, customer_id)\")\n",
    "\n",
    "# ======================================\n",
    "# STEP 5: VERIFY AND SUMMARIZE\n",
    "# ======================================\n",
    "print(\"\\n‚úÖ Step 5: Verification...\")\n",
    "\n",
    "final_count = spark.table(BRONZE_RATINGS_TABLE).count()\n",
    "print(f\"  Total records in Bronze: {final_count:,}\")\n",
    "\n",
    "# Show rating distribution\n",
    "print(\"\\nüìä Rating Distribution:\")\n",
    "spark.table(BRONZE_RATINGS_TABLE) \\\n",
    "    .groupBy(\"rating\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"rating\") \\\n",
    "    .show()\n",
    "    \n",
    "# Show review stats\n",
    "print(\"üìä Review Statistics:\")\n",
    "spark.table(BRONZE_RATINGS_TABLE) \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_ratings\"),\n",
    "        F.sum(\n",
    "            F.when(F.col(\"has_review\").cast(\"boolean\"), 1).otherwise(0)\n",
    "        ).alias(\"with_reviews\"),\n",
    "        F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "        F.avg(\"helpful_votes\").alias(\"avg_helpful_votes\")\n",
    "    ) \\\n",
    "    .show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ü•â BRONZE RATINGS INGESTION COMPLETE!\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_transactions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
